# rag_afsim_system_fixed.py
import os
import torch
from typing import List, Dict, Any
import numpy as np
from chromadb import PersistentClient
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer

class AFSIMRAGSystem:
    def __init__(self, 
                 model_path: str = "D:/Qwen/Qwen/Qwen3-4B",
                 embedding_model: str = "BAAI/bge-small-zh-v1.5",
                 chroma_db_path: str = "./chroma_db"):
        """
        åˆå§‹åŒ–AFSIM RAGç³»ç»Ÿ
        """
        print("æ­£åœ¨åˆå§‹åŒ–AFSIM RAGç³»ç»Ÿ...")
        self.model_path = model_path
        self.embedding_model_name = embedding_model
        self.chroma_db_path = chroma_db_path
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_embedding_model()
        self._init_vector_db()
        self._init_llm()
        
        print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        
    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        print(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(self.embedding_model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        print(f"åµŒå…¥ç»´åº¦: {self.embedding_dim}")
        
    def _init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        print(f"åˆå§‹åŒ–Chromaæ•°æ®åº“: {self.chroma_db_path}")
        self.client = PersistentClient(
            path=self.chroma_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self.client.get_or_create_collection(
            name="afsim_tutorials",
            metadata={"description": "AFSIMæ•™ç¨‹æ–‡æ¡£å‘é‡å­˜å‚¨"}
        )
        
        print(f"æ•°æ®åº“æ–‡æ¡£æ•°é‡: {self.collection.count()}")
        
    def _init_llm(self):
        """åˆå§‹åŒ–Qwen3-4Bæ¨¡å‹"""
        print(f"åŠ è½½Qwen3-4Bæ¨¡å‹: {self.model_path}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # å°è¯•ä½¿ç”¨é‡åŒ–åŠ è½½
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    load_in_4bit=True
                )
                print("âœ“ ä½¿ç”¨4-bité‡åŒ–åŠ è½½æ¨¡å‹")
            except:
                print("âš  é‡åŒ–åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¨ç²¾åº¦åŠ è½½")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float32,
                    device_map="auto"
                )
            
            # è®¾ç½®ç”Ÿæˆå‚æ•°
            self.generation_config = {
                "max_new_tokens": 512,
                "temperature": 0.3,
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_documents_from_folder(self, folder_path):
        """
        ä»æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰.mdæ–‡ä»¶åˆ°å‘é‡æ•°æ®åº“
        """
        print(f"å¼€å§‹æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        
        if not os.path.exists(folder_path):
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return False
        
        if not os.path.isdir(folder_path):
            print(f"âŒ è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")
            
            return False
        
        try:
            # æ‰«ææ‰€æœ‰.mdæ–‡ä»¶
            md_files = []
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.endswith('.md'):
                        full_path = os.path.join(root, file)
                        md_files.append(full_path)
            
            print(f"æ‰¾åˆ° {len(md_files)} ä¸ª.mdæ–‡ä»¶")
            
            if not md_files:
                print("âš  æœªæ‰¾åˆ°ä»»ä½•.mdæ–‡ä»¶")
                return False
            
            documents = []
            metadatas = []
            ids = []
            
            # è¯»å–æ¯ä¸ª.mdæ–‡ä»¶
            for file_path in md_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        doc_content = f.read()
                    
                    if not doc_content.strip():
                        print(f"âš  æ–‡ä»¶å†…å®¹ä¸ºç©º: {os.path.basename(file_path)}")
                        continue
                    
                    # åˆ†å‰²æ–‡æ¡£
                    paragraphs = self._split_into_chunks(doc_content)
                    
                    for i, para in enumerate(paragraphs):
                        if para.strip():  # è·³è¿‡ç©ºæ®µè½
                            doc_id = f"{os.path.basename(file_path)}_{i}"
                            documents.append(para)
                            metadatas.append({
                                "source": file_path,
                                "paragraph": i,
                                "filename": os.path.basename(file_path)
                            })
                            ids.append(doc_id)
                    
                    print(f"âœ“ å·²åŠ è½½: {os.path.basename(file_path)} ({len(paragraphs)} æ®µè½)")
                    
                except Exception as e:
                    print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            # æ‰¹é‡åµŒå…¥å¹¶å­˜å‚¨
            if documents:
                print(f"æ­£åœ¨ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—çš„å‘é‡...")
                embeddings = self.embedding_model.encode(
                    documents,
                    show_progress_bar=True,
                    normalize_embeddings=True,
                    batch_size=32,
                    convert_to_numpy=True
                )
                
                print("æ­£åœ¨å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
                
                # åˆ†æ‰¹å­˜å‚¨ï¼Œé¿å…å†…å­˜é—®é¢˜
                batch_size = 100
                for i in range(0, len(documents), batch_size):
                    end_idx = min(i + batch_size, len(documents))
                    
                    self.collection.add(
                        embeddings=embeddings[i:end_idx].tolist(),
                        documents=documents[i:end_idx],
                        metadatas=metadatas[i:end_idx],
                        ids=ids[i:end_idx]
                    )
                    
                    print(f"  å·²å­˜å‚¨ {end_idx}/{len(documents)} ä¸ªæ–‡æ¡£å—")
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£å—")
                return True
            else:
                print("âš  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def load_documents_from_list(self, file_list_path: str, base_dir: str = "."):
        """
        ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½æ–‡æ¡£ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        print(f"ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½æ–‡æ¡£: {file_list_path}")
        
        if not os.path.exists(file_list_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_list_path}")
            return False
        
        try:
            with open(file_list_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            documents = []
            metadatas = []
            ids = []
            
            for line in lines:
                line = line.strip()
                if line.endswith('.md'):
                    # æ¸…ç†è·¯å¾„
                    file_path = line.replace('D:.\\', '').replace('D:.', '').strip()
                    file_path = file_path.replace('\\', '/')
                    
                    # æ·»åŠ åŸºç¡€ç›®å½•
                    if not os.path.isabs(file_path):
                        file_path = os.path.join(base_dir, file_path)
                    
                    if os.path.exists(file_path):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                doc_content = f.read()
                            
                            paragraphs = self._split_into_chunks(doc_content)
                            
                            for i, para in enumerate(paragraphs):
                                if para.strip():
                                    doc_id = f"{os.path.basename(file_path)}_{i}"
                                    documents.append(para)
                                    metadatas.append({
                                        "source": file_path,
                                        "paragraph": i,
                                        "filename": os.path.basename(file_path)
                                    })
                                    ids.append(doc_id)
                            
                            print(f"âœ“ å·²åŠ è½½: {os.path.basename(file_path)} ({len(paragraphs)} æ®µè½)")
                            
                        except Exception as e:
                            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    else:
                        print(f"âš  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            if documents:
                print(f"æ­£åœ¨ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—çš„å‘é‡...")
                embeddings = self.embedding_model.encode(
                    documents,
                    show_progress_bar=True,
                    normalize_embeddings=True
                )
                
                print("æ­£åœ¨å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
                self.collection.add(
                    embeddings=embeddings.tolist(),
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£å—")
                return True
            else:
                print("âš  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def _split_into_chunks(self, text: str, chunk_size: int = 400) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            if len(current_chunk) + len(para) + 2 <= chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def retrieve_relevant_docs(self, query: str, n_results: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if self.collection.count() == 0:
            print("âš  å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆåŠ è½½æ–‡æ¡£")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(
                query,
                normalize_embeddings=True
            ).tolist()
            
            # æ£€ç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            retrieved_docs = []
            if results['documents']:
                for i, doc in enumerate(results['documents'][0]):
                    retrieved_docs.append({
                        'content': doc,
                        'metadata': results['metadatas'][0][i],
                        'distance': results['distances'][0][i] if results['distances'] else None
                    })
            
            return retrieved_docs
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def format_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:
        """æ ¼å¼åŒ–æç¤ºè¯"""
        if not retrieved_docs:
            return f"""ä½ æ˜¯ä¸€ä¸ªAFSIMï¼ˆAdvanced Framework for Simulationï¼‰ä¸“å®¶åŠ©æ‰‹ã€‚
è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "ä»¥ä¸‹æ˜¯ç›¸å…³çš„AFSIMæ•™ç¨‹å†…å®¹ï¼š\n\n"
        for i, doc in enumerate(retrieved_docs, 1):
            context += f"ã€æ–‡æ¡£{i}ã€‘{doc['metadata']['filename']}\n"
            context += f"{doc['content'][:800]}\n\n"
        
        # å®Œæ•´æç¤º
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªAFSIMï¼ˆAdvanced Framework for Simulationï¼‰ä¸“å®¶åŠ©æ‰‹ã€‚
è¯·åŸºäºæä¾›çš„æ•™ç¨‹å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ•™ç¨‹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ã€‚

é—®é¢˜ï¼š{query}

{context}
è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""
        
        return prompt
    
    def generate_response(self, query: str) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        print(f"\nå¤„ç†æŸ¥è¯¢: {query[:50]}...")
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_relevant_docs(query)
        
        if not retrieved_docs:
            print("âš  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°†åŸºäºæ¨¡å‹çŸ¥è¯†å›ç­”")
        
        # æ„å»ºæç¤º
        prompt = self.format_prompt(query, retrieved_docs)
        
        try:
            # ç”Ÿæˆå›ç­”
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **self.generation_config
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„å›ç­”ï¼ˆå»æ‰æç¤ºéƒ¨åˆ†ï¼‰
            if prompt in response:
                response = response[len(prompt):].strip()
            
            # æ¸…ç†å“åº”
            response = self._clean_response(response)
            
            # æå–æ¥æºä¿¡æ¯
            sources = list(set([doc['metadata']['filename'] for doc in retrieved_docs]))
            
            print(f"âœ“ å›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(response)} å­—ç¬¦")
            
            return {
                "response": response,
                "sources": sources,
                "raw_docs": retrieved_docs
            }
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "response": f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}",
                "sources": [],
                "raw_docs": []
            }
    
    def _clean_response(self, text: str) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line:
                cleaned_lines.append(line)
        
        # é™åˆ¶æœ€å¤§é•¿åº¦
        cleaned_text = '\n'.join(cleaned_lines)
        if len(cleaned_text) > 2000:
            cleaned_text = cleaned_text[:2000] + "...\n\n(å›ç­”è¿‡é•¿ï¼Œå·²æˆªæ–­)"
        
        return cleaned_text
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("\n" + "="*60)
        print("AFSIM RAG ç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
        print("="*60)
        print("å‘½ä»¤:")
        print("  'exit' æˆ– 'quit' - é€€å‡º")
        print("  'clear' - æ¸…ç©ºä¸Šä¸‹æ–‡")
        print("  'sources' - æ˜¾ç¤ºå½“å‰æ¥æº")
        print("  'reload' - é‡æ–°åŠ è½½æ–‡æ¡£")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
                
                if user_input.lower() in ['exit', 'quit']:
                    print("å†è§ï¼")
                    break
                elif user_input.lower() == 'clear':
                    print("ä¸Šä¸‹æ–‡å·²æ¸…ç©º")
                    continue
                elif user_input.lower() == 'sources':
                    print(f"æ•°æ®åº“ä¸­æœ‰ {self.collection.count()} ä¸ªæ–‡æ¡£å—")
                    continue
                elif user_input.lower() == 'reload':
                    print("é‡æ–°åŠ è½½æ–‡æ¡£...")
                    self.load_documents_from_folder("tutorials")
                    continue
                elif not user_input:
                    continue
                
                # ç”Ÿæˆå›ç­”
                result = self.generate_response(user_input)
                
                print(f"\nğŸ¤– AFSIMåŠ©æ‰‹:")
                print("-"*40)
                print(result["response"])
                print("-"*40)
                if result["sources"]:
                    print("å‚è€ƒæ¥æº:")
                    for source in result["sources"]:
                        print(f"  â€¢ {source}")
                print("="*60)
                
            except KeyboardInterrupt:
                print("\nç¨‹åºå·²ä¸­æ–­")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()