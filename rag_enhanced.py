# rag_enhanced.py (å®Œæ•´ä¼˜åŒ–ç‰ˆ)
import os
import warnings
import threading
import time
import logging
import collections
from typing import List, Dict, Any
from functools import lru_cache

import torch
from tqdm import tqdm

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM
)

from project_learner import AFSIMProjectLearner
from utils import ConfigManager, FileReader, InputStateManager, CodeValidator, setup_logging

class EnhancedRAGChatSystem:
    def __init__(self, 
                 project_root: str,
                 model_path: str = None,
                 documents_path: str = None,
                 embedding_model: str = None,
                 vector_db_dir: str = None):
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        self.config = ConfigManager()
        self.project_root = project_root
        self.documents_path = documents_path or project_root
        self.vector_db_dir = vector_db_dir or self.config.get('vector_db.persist_dir')
        self.model_path = model_path or self.config.get('model.path')
        self.embedding_model = embedding_model or self.config.get('embedding.model')
        
        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
        self.input_state = InputStateManager(max_buffer_size=50)
        
        self.logger.info("æ­£åœ¨åˆå§‹åŒ–AFSIMé¡¹ç›®å­¦ä¹ å™¨...")
        self.project_learner = AFSIMProjectLearner(project_root)
        self.project_learner.analyze_project_structure()
        
        self.logger.info("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embeddings = self._setup_embeddings()
        
        self.logger.info("æ­£åœ¨åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
        self.tokenizer, self.model = self._setup_llm()
        
        # æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“
        self.vector_db = self.build_or_load_vector_db()
        self.enhanced_qa_chain = self.create_enhanced_qa_chain()
        
        self.conversation_history = []
        
        self.logger.info("EnhancedRAGChatSystem åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_embeddings(self):
        """è®¾ç½®åµŒå…¥æ¨¡å‹"""
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        try:
            return HuggingFaceBgeEmbeddings(
                model_name=self.embedding_model,
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True},
                query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
            )
        except Exception as e:
            self.logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return HuggingFaceBgeEmbeddings(
                model_name=self.embedding_model,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True},
                query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
            )
    
    def _setup_llm(self):
        """è®¾ç½®è¯­è¨€æ¨¡å‹"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True,
                padding_side='left'
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # æ ¹æ®å¯ç”¨è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼
            if torch.cuda.is_available():
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    dtype=torch.float16,
                    device_map="cuda",
                    trust_remote_code=True,
                ).eval()
            
            return tokenizer, model
                
        except Exception as e:
            self.logger.error(f"åŠ è½½è¯­è¨€æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    @lru_cache(maxsize=100)
    def search_similar_documents_cached(self, query: str, top_k: int = None):
        """å¸¦ç¼“å­˜çš„æ–‡æ¡£æœç´¢"""
        if top_k is None:
            top_k = self.config.get('vector_db.search_k', 6)
        return self.vector_db.similarity_search(query, k=top_k)
    
    def build_or_load_vector_db(self):
        """æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“"""
        if os.path.exists(self.vector_db_dir):
            self.logger.info("åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“...")
            try:
                vector_db = Chroma(
                    persist_directory=self.vector_db_dir,
                    embedding_function=self.embeddings
                )
                # éªŒè¯æ•°æ®åº“æ˜¯å¦æœ‰æ•ˆ
                if hasattr(vector_db, '_collection') and vector_db._collection.count() > 0:
                    self.logger.info("å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
                    return vector_db
                else:
                    self.logger.warning("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œå°†é‡æ–°æ„å»º")
            except Exception as e:
                self.logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼Œå°†é‡æ–°æ„å»º: {e}")
        
        self.logger.info("æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
        return self.build_knowledge_base()
    
    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“ - ä¿®å¤æ‰¹é‡å¤§å°é—®é¢˜"""
        self.logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“...")
        start_time = time.time()

        try:
            # æ”¶é›†æ‰€æœ‰å¯è¯»çš„æ–‡ä»¶
            all_txt_files = []
            for root, dirs, files in os.walk(self.documents_path):
                for file in files:
                    if file.endswith('.txt'):
                        file_path = os.path.join(root, file)
                        if not FileReader.should_skip_file(file_path):
                            all_txt_files.append(file_path)
            
            self.logger.info(f"æ‰¾åˆ° {len(all_txt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")
            
            documents = []
            skipped_files = 0
            
            # ä½¿ç”¨è¿›åº¦æ¡
            with tqdm(total=len(all_txt_files), desc="åŠ è½½æ–‡ä»¶") as pbar:
                for file_path in all_txt_files:
                    try:
                        content, encoding = FileReader.read_file_safely(file_path)
                        if content and content.strip():
                            from langchain_core.documents import Document
                            doc = Document(
                                page_content=content,
                                metadata={"source": file_path, "encoding": encoding}
                            )
                            documents.append(doc)
                        else:
                            skipped_files += 1
                    except Exception as e:
                        self.logger.warning(f"è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶: {file_path} - {e}")
                        skipped_files += 1
                    finally:
                        pbar.update(1)
                        pbar.set_postfix({"å½“å‰æ–‡ä»¶": os.path.basename(file_path)})
            
            if not documents:
                raise ValueError(f"åœ¨ {self.documents_path} ç›®å½•ä¸­æœªæ‰¾åˆ°å¯è¯»æ–‡æ¡£")
            
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£ (è·³è¿‡äº† {skipped_files} ä¸ªæ–‡ä»¶)")

            # åˆ†å‰²æ–‡æ¡£
            chunk_size = self.config.get('vector_db.chunk_size')
            chunk_overlap = self.config.get('vector_db.chunk_overlap')
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]
            )
            
            with tqdm(total=len(documents), desc="åˆ†å‰²æ–‡æ¡£") as pbar:
                texts = []
                for doc in documents:
                    chunks = text_splitter.split_documents([doc])
                    texts.extend(chunks)
                    pbar.update(1)
            
            self.logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå¾—åˆ° {len(texts)} ä¸ªæ–‡æœ¬å—")

            # ä¿®å¤ï¼šåˆ†æ‰¹æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.logger.info("åˆ†æ‰¹åˆ›å»ºå‘é‡æ•°æ®åº“...")
            vector_db = self._create_vector_db_in_batches(texts)
            
            end_time = time.time()
            self.logger.info(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œè€—æ—¶ {(end_time - start_time)/60:.2f} åˆ†é’Ÿ")
            return vector_db
            
        except Exception as e:
            self.logger.error(f"æ„å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            raise

    def _create_vector_db_in_batches(self, texts, batch_size=4000):
        """åˆ†æ‰¹åˆ›å»ºå‘é‡æ•°æ®åº“ä»¥é¿å…æ‰¹é‡å¤§å°é™åˆ¶"""
        from langchain_community.vectorstores import Chroma
        
        self.logger.info(f"å¼€å§‹åˆ†æ‰¹å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£å—ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # ç¬¬ä¸€æ¬¡æ‰¹æ¬¡åˆ›å»ºæ•°æ®åº“
        first_batch = texts[:batch_size]
        self.logger.info(f"åˆ›å»ºåˆå§‹å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(first_batch)} ä¸ªæ–‡æ¡£...")
        
        vector_db = Chroma.from_documents(
            documents=first_batch,
            embedding=self.embeddings,
            persist_directory=self.vector_db_dir
        )
        
        # å‰©ä½™æ‰¹æ¬¡é€æ­¥æ·»åŠ 
        remaining_texts = texts[batch_size:]
        if remaining_texts:
            self.logger.info(f"é€æ­¥æ·»åŠ å‰©ä½™ {len(remaining_texts)} ä¸ªæ–‡æ¡£...")
            
            for i in range(0, len(remaining_texts), batch_size):
                batch = remaining_texts[i:i + batch_size]
                self.logger.info(f"æ·»åŠ æ‰¹æ¬¡ {i//batch_size + 1}/{(len(remaining_texts)-1)//batch_size + 1}ï¼ŒåŒ…å« {len(batch)} ä¸ªæ–‡æ¡£")
                
                vector_db.add_documents(batch)
                
                # åŠæ—¶æ¸…ç†å†…å­˜
                del batch
        
        # æŒä¹…åŒ–æœ€ç»ˆæ•°æ®åº“
        vector_db.persist()
        self.logger.info("å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆå¹¶å·²æŒä¹…åŒ–")
        
        return vector_db
    
    def create_enhanced_qa_chain(self):
        """åˆ›å»ºå¢å¼ºçš„QAé“¾ï¼ŒåŒ…å«é¡¹ç›®ä¸Šä¸‹æ–‡"""
        
        class CustomQwenLLM:
            def __init__(self, model, tokenizer, project_learner):
                self.model = model
                self.tokenizer = tokenizer
                self.project_learner = project_learner
                self.config = ConfigManager()
                self.logger = logging.getLogger(__name__)
            
            def __call__(self, prompt):
                try:
                    inputs = self.tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=32000,
                        padding=True
                    )
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=self.config.get('model.max_tokens'),
                            do_sample=True,
                            temperature=self.config.get('model.temperature'),
                            top_p=self.config.get('model.top_p'),
                            top_k=self.config.get('model.top_k'),
                            pad_token_id=self.tokenizer.eos_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            repetition_penalty=self.config.get('model.repetition_penalty'),
                            num_return_sequences=1,
                            use_cache=True  # å¯ç”¨KVç¼“å­˜
                        )
                    
                    response = self.tokenizer.decode(
                        outputs[0][inputs['input_ids'].shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return response
                    
                except Exception as e:
                    self.logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
                    return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
        
        # åˆ›å»ºæ£€ç´¢å™¨
        search_k = self.config.get('vector_db.search_k')
        retriever = self.vector_db.as_retriever(search_kwargs={"k": search_k})
        
        class EnhancedCodeGenerationChain:
            def __init__(self, llm, retriever, project_learner):
                self.llm = llm
                self.retriever = retriever
                self.project_learner = project_learner
                self.validator = CodeValidator()
                self.logger = logging.getLogger(__name__)
            
            def run(self, query):
                try:
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
                    docs = self.retriever.get_relevant_documents(query)
                    
                    # è·å–é¡¹ç›®ä¸Šä¸‹æ–‡
                    project_context = self.project_learner.generate_context_prompt(query)
                    
                    # æ„å»ºå¢å¼ºçš„æç¤ºè¯
                    context = self._build_context_prompt(project_context, docs)
                    prompt = self._build_generation_prompt(context, query)
                    
                    # ç”Ÿæˆå›ç­”
                    response = self.llm(prompt)
                    
                    # éªŒè¯ç”Ÿæˆçš„ä»£ç 
                    validation_result = self.validator.validate_afsim_code(response)
                    
                    return {
                        "result": response,
                        "source_documents": docs,
                        "project_context": project_context,
                        "validation": validation_result
                    }
                    
                except Exception as e:
                    self.logger.error(f"è¿è¡Œå¢å¼ºä»£ç ç”Ÿæˆé“¾æ—¶å‡ºé”™: {e}")
                    return {
                        "result": f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™: {str(e)}",
                        "source_documents": [],
                        "project_context": "",
                        "validation": {"is_valid": False, "errors": [str(e)]}
                    }
            
            def _build_context_prompt(self, project_context, docs):
                """æ„å»ºä¸Šä¸‹æ–‡æç¤ºè¯"""
                context = " AFSIMé¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯:\n\n"
                context += project_context
                context += "\n\n ç›¸å…³ä»£ç ç¤ºä¾‹:\n\n"
                
                for i, doc in enumerate(docs, 1):
                    source_info = f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}" if hasattr(doc, 'metadata') else ""
                    context += f"ç¤ºä¾‹ {i} {source_info}:\n{doc.page_content}\n{'='*50}\n"
                
                return context
            
            def _clean_query(self, query: str) -> str:
                """æ¸…ç†æŸ¥è¯¢ä¸­çš„é‡å¤å†…å®¹"""
                import re
                
                # ç§»é™¤è¿‡å¤šçš„é‡å¤è¦æ±‚
                lines = query.split('\n')
                cleaned_lines = []
                required_keywords_seen = set()
                
                for line in lines:
                    line_clean = line.strip()
                    if not line_clean:
                        continue
                        
                    # æ£€æµ‹é‡å¤çš„"å¿…é¡»åŒ…å«"æ¨¡å¼
                    if "å¿…é¡»åŒ…å«" in line_clean:
                        # æå–å…³é”®å†…å®¹
                        key_content = re.sub(r'.*å¿…é¡»åŒ…å«', '', line_clean).strip()
                        if key_content and key_content not in required_keywords_seen:
                            cleaned_lines.append(line_clean)
                            required_keywords_seen.add(key_content)
                    else:
                        cleaned_lines.append(line_clean)
                
                # å¦‚æœæ¸…ç†åå†…å®¹å¤ªå°‘ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢çš„é‡è¦éƒ¨åˆ†
                if len(cleaned_lines) < 2:
                    # æå–åŸå§‹æŸ¥è¯¢ä¸­çš„å…³é”®è¡Œ
                    important_lines = []
                    for line in lines[:10]:  # åªå–å‰10è¡Œé¿å…é‡å¤
                        line_clean = line.strip()
                        if line_clean and "å¿…é¡»åŒ…å«" not in line_clean:
                            important_lines.append(line_clean)
                    return "\n".join(important_lines[:5])
                
                return "\n".join(cleaned_lines[:20])  # é™åˆ¶é•¿åº¦
            
            def _build_generation_prompt(self, context, query):
                """æ„å»ºç”Ÿæˆæç¤ºè¯"""
                cleaned_query = self._clean_query(query)

                return f"""ä½ æ˜¯ä¸€ä¸ªAFSIMä»£ç ç”Ÿæˆä¸“å®¶ï¼Œç†Ÿæ‚‰æ•´ä¸ªé¡¹ç›®ç»“æ„å’ŒåŸºç¡€åº“çš„ä½¿ç”¨ã€‚
{context}
 ç”¨æˆ·éœ€æ±‚: {query}
è¯·åŸºäºä»¥ä¸Šé¡¹ç›®ç»“æ„å’Œç›¸å…³ä»£ç ç¤ºä¾‹ï¼Œç›´æ¥ç”Ÿæˆå‡†ç¡®ã€å®Œæ•´çš„AFSIMä»£ç ã€‚
 ç¦æ­¢:
ä¸è¦æ·»åŠ è§£é‡Šæ€§æ–‡å­—
ä¸è¦é‡å¤ç›¸åŒçš„å†…å®¹
ä¸è¦è¾“å‡ºä¸å®Œæ•´çš„ä»£ç å—
è¯·ç”Ÿæˆå®Œæ•´çš„AFSIMä»£ç :"""
        
        return EnhancedCodeGenerationChain(
            llm=CustomQwenLLM(self.model, self.tokenizer, self.project_learner),
            retriever=retriever,
            project_learner=self.project_learner
        )
    
        
    
    def generate_enhanced_response(self, query: str) -> Dict[str, Any]:
        """ç”Ÿæˆå¢å¼ºçš„å›ç­”"""
        try:
            # ä½¿ç”¨å¢å¼ºçš„QAé“¾ç”Ÿæˆå›ç­”
            result = self.enhanced_qa_chain.run(query)
            
            # æ›´æ–°å¯¹è¯å†å²
            self.conversation_history.append({
                'query': query,
                'response': result["result"],
                'sources': len(result["source_documents"]),
                'validation': result["validation"],
                'timestamp': time.time()
            })
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            if len(self.conversation_history) > 8:
                self.conversation_history = self.conversation_history[-6:]
            
            return result
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¢å¼ºå“åº”æ—¶å‡ºé”™: {e}")
            return {
                "result": f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}", 
                "source_documents": [],
                "project_context": "",
                "validation": {"is_valid": False, "errors": [str(e)]}
            }
    
    def get_vector_db_info(self):
        """è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯"""
        if hasattr(self.vector_db, '_collection'):
            count = self.vector_db._collection.count()
            return f"å‘é‡æ•°æ®åº“åŒ…å« {count} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
        return "å‘é‡æ•°æ®åº“ä¿¡æ¯ä¸å¯ç”¨"
    
    def get_project_info(self):
        """è·å–é¡¹ç›®ä¿¡æ¯"""
        return self.project_learner.get_project_summary()
    
    def search_project_files(self, keyword: str, max_results: int = 5):
        """åœ¨é¡¹ç›®ä¸­æœç´¢æ–‡ä»¶"""
        return self.project_learner.find_related_files(keyword, max_results)
    
    def __del__(self):
        """ææ„å‡½æ•°é‡Šæ”¾èµ„æº"""
        self.logger.info("æ¸…ç†ç³»ç»Ÿèµ„æº...")
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'vector_db'):
            del self.vector_db
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    


class EnhancedInputHandler:
    """ä¼˜åŒ–ç‰ˆçš„è¾“å…¥å¤„ç†å™¨"""
    
    def __init__(self, chat_system: EnhancedRAGChatSystem):
        self.chat_system = chat_system
        self.termination_shortcut = "//end"
        self.help_shortcut = "//help"
        self.info_shortcut = "//info"
        self.search_shortcut = "//search"
        self.clear_shortcut = "//clear"
        self.history_shortcut = "//history"
        self.logger = logging.getLogger(__name__)
        
    def start_input_listener(self):
        """å¯åŠ¨è¾“å…¥ç›‘å¬çº¿ç¨‹"""
        input_thread = threading.Thread(target=self._input_loop, daemon=True)
        input_thread.start()
        self.logger.info("è¾“å…¥ç›‘å¬å™¨å·²å¯åŠ¨")
        
    def _input_loop(self):
        """è¾“å…¥å¾ªç¯"""
        self._show_welcome_message()
        
        while True:
            try:
                line = input().strip()
                
                if line == self.termination_shortcut:
                    self._process_complete_query()
                elif line == self.help_shortcut:
                    self._show_help()
                elif line == self.info_shortcut:
                    self._show_system_info()
                elif line == self.clear_shortcut:
                    self._clear_input()
                elif line == self.history_shortcut:
                    self._show_conversation_history()
                elif line.startswith(f"{self.search_shortcut} "):
                    keyword = line[len(self.search_shortcut)+1:]
                    self._search_files(keyword)
                elif line:
                    # ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨æ·»åŠ è¾“å…¥
                    if not self.chat_system.input_state.add_input(line):
                        print("âš ï¸  è¾“å…¥ç¼“å†²åŒºå·²æ»¡ï¼Œè¯·å…ˆå¤„ç†å½“å‰è¾“å…¥æˆ–ä½¿ç”¨ //clear æ¸…ç©º")
                else:
                    # ç©ºè¡Œï¼Œå¿½ç•¥
                    continue
                    
            except KeyboardInterrupt:
                self.logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
                print("\n\nç¨‹åºé€€å‡ºã€‚")
                break
            except Exception as e:
                self.logger.error(f"è¾“å…¥é”™è¯¯: {e}")
                print(f"è¾“å…¥é”™è¯¯: {e}")
    
    def _show_welcome_message(self):
        """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
        print(f"\n{'='*80}")
        print(" AFSIM æ™ºèƒ½ä»£ç ç”Ÿæˆç³»ç»Ÿ (å¢å¼ºä¼˜åŒ–ç‰ˆ)")
        print(f"{'='*80}")
        print(f"è¯·è¾“å…¥AFSIMä»£ç ç”Ÿæˆéœ€æ±‚ï¼ˆæ”¯æŒå¤šè¡Œè¾“å…¥ï¼‰")
        print(f"è¾“å…¥ '{self.termination_shortcut}' ç»“æŸè¾“å…¥å¹¶ç”Ÿæˆä»£ç ")
        print(f"è¾“å…¥ '{self.help_shortcut}' æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print(f"è¾“å…¥ '{self.info_shortcut}' æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯")
        print(f"è¾“å…¥ '{self.search_shortcut} å…³é”®è¯' æœç´¢é¡¹ç›®æ–‡ä»¶")
        print(f"è¾“å…¥ '{self.clear_shortcut}' æ¸…ç©ºå½“å‰è¾“å…¥")
        print(f"è¾“å…¥ '{self.history_shortcut}' æ˜¾ç¤ºå¯¹è¯å†å²")
        print(f"{'-'*80}")
        self._show_example_queries()
    
    def _show_example_queries(self):
        """æ˜¾ç¤ºç¤ºä¾‹æŸ¥è¯¢"""
        print("\n ç¤ºä¾‹ä»£ç ç”Ÿæˆéœ€æ±‚ï¼š")
        examples = [
            "ä½¿ç”¨base_typesåº“åˆ›å»ºä¸€ä¸ªæˆ˜æ–—æœºå¹³å°ï¼ŒåŒ…å«é›·è¾¾å’Œå¯¼å¼¹",
            "åŸºäºweaponsåº“é…ç½®ç©ºå¯¹ç©ºå¯¼å¼¹ç³»ç»Ÿ",
            "ä½¿ç”¨sensorsåº“æ·»åŠ é›·è¾¾æ¢æµ‹å’Œè·Ÿè¸ªåŠŸèƒ½", 
            "åˆ›å»ºåŒ…å«å¤šä¸ªå¹³å°çš„å¤æ‚ç©ºæˆ˜åœºæ™¯",
            "å®ç°å¯¼å¼¹çš„åˆ¶å¯¼å’Œæ§åˆ¶ç³»ç»Ÿ",
            "é…ç½®å¹³å°çš„é£è¡Œè¡Œä¸ºå’Œæˆ˜æœ¯"
        ]
        for i, example in enumerate(examples, 1):
            print(f"  {i}. {example}")
        print(f"{'-'*80}\n")
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print(f"\n{'='*50}")
        print("å¸®åŠ©ä¿¡æ¯:")
        print(f"{'='*50}")
        print(f"  {self.termination_shortcut} - ç»“æŸè¾“å…¥å¹¶ç”Ÿæˆä»£ç ")
        print(f"  {self.help_shortcut} - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯") 
        print(f"  {self.info_shortcut} - æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯")
        print(f"  {self.search_shortcut} å…³é”®è¯ - æœç´¢é¡¹ç›®ä¸­çš„æ–‡ä»¶")
        print(f"  {self.clear_shortcut} - æ¸…ç©ºå½“å‰è¾“å…¥ç¼“å†²åŒº")
        print(f"  {self.history_shortcut} - æ˜¾ç¤ºå¯¹è¯å†å²")
        print(f"\næç¤º:")
        print("  - å¯ä»¥è¾“å…¥å¤šè¡Œéœ€æ±‚ï¼Œç³»ç»Ÿä¼šç»¼åˆåˆ†æ")
        print("  - å°½é‡è¯¦ç»†æè¿°éœ€æ±‚ï¼ŒåŒ…æ‹¬å¹³å°ç±»å‹ã€æ­¦å™¨é…ç½®ç­‰")
        print("  - ç³»ç»Ÿä¼šè‡ªåŠ¨å‚è€ƒé¡¹ç›®ä¸­çš„åŸºç¡€åº“å’Œç°æœ‰ä»£ç ")
        print("  - è¾“å…¥ç¼“å†²åŒºé™åˆ¶ä¸º50è¡Œï¼Œé¿å…å†…å­˜æº¢å‡º")
        print(f"{'='*50}\n")
        self.chat_system.input_state.is_processing = False
    
    def _show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print(f"\n{'='*50}")
        print("ç³»ç»Ÿä¿¡æ¯:")
        print(f"{'='*50}")
        
        # é¡¹ç›®ä¿¡æ¯
        project_info = self.chat_system.get_project_info()
        print(f"é¡¹ç›®ç»Ÿè®¡:")
        print(f"  - æ€»æ–‡ä»¶æ•°: {project_info['total_files']}")
        print(f"  - åŸºç¡€åº“: {', '.join(project_info['base_libraries'])}")
        print(f"  - ä»£ç æ¨¡å—: {len(project_info['code_modules'])} ä¸ª")
        print(f"  - æ–‡ä»¶åˆ†ç±»: {project_info['file_categories']}")
        
        # æ•°æ®åº“ä¿¡æ¯
        db_info = self.chat_system.get_vector_db_info()
        print(f"çŸ¥è¯†åº“: {db_info}")
        
        # å¯¹è¯å†å²
        print(f"å¯¹è¯å†å²: {len(self.chat_system.conversation_history)} æ¡è®°å½•")
        
        # è¾“å…¥ç¼“å†²åŒºçŠ¶æ€
        inputs = self.chat_system.input_state.get_all_inputs()
        print(f"å½“å‰è¾“å…¥ç¼“å†²åŒº: {len(inputs)} è¡Œ")
        self.chat_system.input_state.clear()  # æ¸…ç©ºä¸´æ—¶è·å–çš„è¾“å…¥
        
        # ç³»ç»ŸçŠ¶æ€
        print(f"å¤„ç†çŠ¶æ€: {'å¤„ç†ä¸­' if self.chat_system.input_state.is_processing else 'ç­‰å¾…è¾“å…¥'}")
        print(f"{'='*50}\n")
        self.chat_system.input_state.is_processing = False
    
    def _search_files(self, keyword: str):
        """æœç´¢æ–‡ä»¶"""
        print(f"\nğŸ” æœç´¢æ–‡ä»¶: '{keyword}'")
        results = self.chat_system.search_project_files(keyword, 5)
        if results:
            print("æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³æ–‡ä»¶:")
            for i, file_path in enumerate(results, 1):
                print(f"  {i}. {file_path}")
        else:
            print("æœªæ‰¾åˆ°ç›¸å…³æ–‡ä»¶")
        print()
        self.chat_system.input_state.is_processing = False
    
    def _clear_input(self):
        """æ¸…ç©ºè¾“å…¥ç¼“å†²åŒº"""
        self.chat_system.input_state.clear()
        print("âœ… è¾“å…¥ç¼“å†²åŒºå·²æ¸…ç©º")
        self.chat_system.input_state.is_processing = False
    
    def _show_conversation_history(self):
        """æ˜¾ç¤ºå¯¹è¯å†å²"""
        print(f"\n{'='*20}")
        print("å¯¹è¯å†å²:")
        print(f"{'='*20}")
        
        if not self.chat_system.conversation_history:
            print("æš‚æ— å¯¹è¯å†å²")
        else:
            for i, conv in enumerate(self.chat_system.conversation_history, 1):
                print(f"\nå¯¹è¯ {i} ({(time.time() - conv['timestamp'])/60:.1f}åˆ†é’Ÿå‰):")
                print(f"  é—®é¢˜: {conv['query'][:100]}...")
                print(f"  å›ç­”: {conv['response'][:100]}...")
                print(f"  å‚è€ƒ: {conv['sources']} ä¸ªç¤ºä¾‹")
                if 'validation' in conv:
                    valid_status = "âœ… æœ‰æ•ˆ" if conv['validation']['is_valid'] else "âŒ æœ‰é—®é¢˜"
                    print(f"  éªŒè¯: {valid_status}")
        
        print(f"{'='*20}\n")
        self.chat_system.input_state.is_processing = False
    
    def _process_complete_query(self):
        """å¤„ç†å®Œæ•´çš„æŸ¥è¯¢"""
        # è·å–æ‰€æœ‰è¾“å…¥
        inputs = self.chat_system.input_state.get_all_inputs()
        
        if not inputs:
            print("âŒ è¾“å…¥ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥éœ€æ±‚")
            self.chat_system.input_state.is_processing = False
            return
        
        full_query = self._clean_and_optimize_query(inputs)
        
        # å°è¯•è·å–å¤„ç†é”
        if not self.chat_system.input_state.acquire_processing_lock():
            print("âš ï¸  ç³»ç»Ÿæ­£åœ¨å¤„ç†å…¶ä»–è¯·æ±‚ï¼Œè¯·ç¨å€™...")
            return
        
        try:
            self.chat_system.input_state.is_processing = True
            
            print(f"\nâ³ æ­£åœ¨ç”ŸæˆAFSIMä»£ç ...")
            start_time = time.time()
            
            result = self.chat_system.generate_enhanced_response(full_query)
            end_time = time.time()
            
            self._display_result(full_query, result, end_time - start_time)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            print(f"âŒ ç”Ÿæˆä»£ç æ—¶å‡ºé”™: {e}")
        finally:
            # ç¡®ä¿çŠ¶æ€æ¢å¤å’Œé”é‡Šæ”¾
            self.chat_system.input_state.is_processing = False
            self.chat_system.input_state.release_processing_lock()
        
        print(f"\n è¯·è¾“å…¥æ–°çš„AFSIMä»£ç ç”Ÿæˆéœ€æ±‚ï¼ˆè¾“å…¥ '{self.help_shortcut}' æŸ¥çœ‹å¸®åŠ©ï¼‰:")
    
    def _display_result(self, query: str, result: Dict, duration: float):
        """æ˜¾ç¤ºç”Ÿæˆç»“æœ"""
        print(f"\n{'âœ…'*40}")
        print(f"ç”Ÿæˆçš„AFSIMä»£ç  (è€—æ—¶: {duration:.2f}ç§’)")
        # print(f" åŸå§‹éœ€æ±‚: {query}")
        # print(f"{'-'*80}")
        # print(f" ç”Ÿæˆçš„ä»£ç :")
        # print(f"{'='*80}")
        print(result["result"])
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºéªŒè¯ç»“æœ
        # if result.get("validation"):
        #     validation = result["validation"]
        #     if validation["is_valid"]:
        #         print("âœ… ä»£ç éªŒè¯: é€šè¿‡")
        #     else:
        #         print("âŒ ä»£ç éªŒè¯: æœªé€šè¿‡")
        #         if validation.get("errors"):
        #             print("   é”™è¯¯:")
        #             for error in validation["errors"]:
        #                 print(f"     - {error}")
        #         if validation.get("warnings"):
        #             print("   è­¦å‘Š:")
        #             for warning in validation["warnings"]:
        #                 print(f"     - {warning}")
        #         if validation.get("suggestions"):
        #             print("   å»ºè®®:")
        #             for suggestion in validation["suggestions"]:
        #                 print(f"     - {suggestion}")
        
        # # æ˜¾ç¤ºå‚è€ƒä¿¡æ¯
        # if result.get("source_documents"):
        #     print(f"\nğŸ“š å‚è€ƒäº† {len(result['source_documents'])} ä¸ªä»£ç ç¤ºä¾‹")
        #     for i, doc in enumerate(result["source_documents"][:3], 1):
        #         source = doc.metadata.get('source', 'æœªçŸ¥') if hasattr(doc, 'metadata') else 'æœªçŸ¥'
        #         print(f"  {i}. {os.path.basename(source)}")
        
        # # æ˜¾ç¤ºé¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
        # if result.get("project_context"):
        #     print(f"\n é¡¹ç›®ä¸Šä¸‹æ–‡: åŸºäº {len(self.chat_system.project_learner.all_files)} ä¸ªæ–‡ä»¶åˆ†æ")
        
        # print(f"{'âœ…'*40}\n")
    
    def _clean_and_optimize_query(self, inputs: List[str]) -> str:
        """æ¸…ç†å’Œä¼˜åŒ–æŸ¥è¯¢è¾“å…¥"""
        full_query = "\n".join(inputs)
        
        # ç§»é™¤é‡å¤çš„è¦æ±‚
        lines = full_query.split('\n')
        unique_lines = []
        seen_lines = set()
        
        for line in lines:
            line_stripped = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œè¿‡äºé‡å¤çš„è¡Œ
            if line_stripped and line_stripped not in seen_lines:
                # ç®€åŒ–é‡å¤çš„"å¿…é¡»åŒ…å«"è¦æ±‚
                if "å¿…é¡»åŒ…å«" in line_stripped and len(line_stripped) < 100:
                    unique_lines.append(line_stripped)
                    seen_lines.add(line_stripped)
                elif "å¿…é¡»åŒ…å«" not in line_stripped:
                    unique_lines.append(line_stripped)
                    seen_lines.add(line_stripped)
        
        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªå°‘ï¼Œä¿ç•™åŸå§‹è¾“å…¥
        if len(unique_lines) < 3:
            return full_query
        
        optimized_query = "\n".join(unique_lines)
        self.logger.info(f"æŸ¥è¯¢ä¼˜åŒ–: {len(lines)} -> {len(unique_lines)} è¡Œ")
        
        return optimized_query
    